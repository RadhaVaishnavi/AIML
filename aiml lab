Experiment name: 
For a given set of training data examples stored in a .csv file, implement and demonstrate the 
candidate elimination algorithm to output a description of the set of all hypotheses consistent with 
the training examples.
Aim: For a given set of training data examples stored in a .csv file, implement and demonstrate the 
candidate elimination algorithm to output a description of the set of all hypotheses consistent with 
the training examples. 
Dataset sample: (enjoy_sport) 
sky,air_temp,humudity,wind,water,forecast,enjoy_sport 
sunny,warm,normal,strong,warm,same,yes 
sunny,warm,high,strong,warm,same,yes 
rainy,cold,high,strong,warm,change,no 
sunny,warm,high,strong,cool,change,yes 
Program and Output: 
In: import numpy as np 
import pandas as pd 
data = pd.read_csv('enjoysport.csv') 
concepts = np.array(data.iloc[:,0:-1]) 
print("\nInstances are:\n",concepts) 
Out: Instances are: 
 [['sunny' 'warm' 'normal' 'strong' 'warm' 'same'] 
 ['sunny' 'warm' 'high' 'strong' 'warm' 'same'] 
 ['rainy' 'cold' 'high' 'strong' 'warm' 'change'] 
 ['sunny' 'warm' 'high' 'strong' 'cool' 'change']] 
target = np.array(data.iloc[:,-1]) 
print("\nTarget Values are: ",target) 
Out: Target Values are: ['yes' 'yes' 'no' 'yes'] 
def learn(concepts, target): 
 specific_h = concepts[0].copy() 
 print("\nInitialization of specific_h and genearal_h") 
 print("\nSpecific Boundary: ", specific_h) 
 general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))] 
 print("\nGeneric Boundary: ",general_h) 
 for i, h in enumerate(concepts): 
 print("\nInstance", i+1 , "is ", h) 
 if target[i] == "yes": 
 print("Instance is Positive ") 
 for x in range(len(specific_h)): 
 if h[x]!= specific_h[x]: 
 specific_h[x] ='?' 
 general_h[x][x] ='?' 
 
 if target[i] == "no": 
 print("Instance is Negative ") 
 for x in range(len(specific_h)): 
 if h[x]!= specific_h[x]: 
 general_h[x][x] = specific_h[x] 
 else: 
 general_h[x][x] = '?' 
 
 print("Specific Bundary after ", i+1, "Instance is ", specific_h) 
 print("Generic Boundary after ", i+1, "Instance is ", general_h) 
 print("\n") 
 indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']] 
 for i in indices: 
 general_h.remove(['?', '?', '?', '?', '?', '?']) 
 return specific_h, general_h 
s_final, g_final = learn(concepts, target) 
print("Final Specific_h: ", s_final, sep="\n") 
print("Final General_h: ", g_final, sep="\n") 
Out: 
Initialization of specific_h and genearal_h 
Specific Boundary: ['sunny' 'warm' 'normal' 'strong' 'warm' 'same'] 
Generic Boundary: [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?',
 '?', '?', '?', '?', '?']] 
Instance 1 is ['sunny' 'warm' 'normal' 'strong' 'warm' 'same'] 
Instance is Positive 
Specific Bundary after 1 Instance is ['sunny' 'warm' 'normal' 'strong' 'warm' 'same'] 
Generic Boundary after 1 Instance is [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?'
, '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']] 
Instance 2 is ['sunny' 'warm' 'high' 'strong' 'warm' 'same'] 
Instance is Positive 
Specific Bundary after 2 Instance is ['sunny' 'warm' '?' 'strong' 'warm' 'same'] 
Generic Boundary after 2 Instance is [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?'
, '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']] 
Instance 3 is ['rainy' 'cold' 'high' 'strong' 'warm' 'change'] 
Instance is Negative 
Specific Bundary after 3 Instance is ['sunny' 'warm' '?' 'strong' 'warm' 'same'] 
Generic Boundary after 3 Instance is [['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '
?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', 'same']] 
Instance 4 is ['sunny' 'warm' 'high' 'strong' 'cool' 'change'] 
Instance is Positive 
Specific Bundary after 4 Instance is ['sunny' 'warm' '?' 'strong' '?' '?'] 
Generic Boundary after 4 Instance is [['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '
?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']] 
Final Specific_h: 
['sunny' 'warm' '?' 'strong' '?' '?'] 
Final General_h: 
[['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?']]



Experiment name: 
Write a program to demonstrate the working of Decision tree Calssifier. Use appropriate dataset for 
building the decision tree and apply this knowledge to classify a new sample. 
Aim: To write a program to demonstrate the working of Decision tree Calssifier. Use appropriate dataset 
for building the decision tree and apply this knowledge to classify a new sample. 
Dataset sample: (salaries) 
company,job,degree,salary_more_then_100k 
google,sales executive,bachelors,0 
google,sales executive,masters,0 
google,business manager,bachelors,1 
google,business manager,masters,1 
google,computer programmer,bachelors,0 
google,computer programmer,masters,1 
abc pharma,sales executive,masters,0 
abc pharma,computer programmer,bachelors,0 
abc pharma,business manager,bachelors,0 
abc pharma,business manager,masters,1 
facebook,sales executive,bachelors,1 
facebook,sales executive,masters,1 
facebook,business manager,bachelors,1 
facebook,business manager,masters,1 
facebook,computer programmer,bachelors,1 
facebook,computer programmer,masters,1 
Program and Output: 
In: import pandas as pd 
In: df = pd.read_csv("salaries.csv") 
In: df.head() 
Out: 
In: inputs = df.drop('salary_more_then_100k',axis='columns') 
In: target = df['salary_more_then_100k'] 
In: from sklearn.preprocessing import LabelEncoder 
 le_company = LabelEncoder() 
 le_job = LabelEncoder() 
 le_degree = LabelEncoder() 
In: inputs['company_n'] = le_company.fit_transform(inputs['company']) 
 inputs['job_n'] = le_job.fit_transform(inputs['job']) 
 inputs['degree_n'] = le_degree.fit_transform(inputs['degree']) 
In: inputs 
Out: 
In: inputs_n = inputs.drop(['company','job','degree'],axis='columns') 
In: inputs_n 
Out: 
In: target 
Out: 
In: from sklearn import tree 
 model = tree.DecisionTreeClassifier() 
In: model.fit(inputs_n, target) 
Out: DecisionTreeClassifier() 
In: model.score(inputs_n,target) 
Out: 1.0 
In: model.predict([[2,0,1]]) 
Out: array([1], dtype=int64)



Experiment name: 
Write a program to demonstrate the working of Decision tree Regressor. Use appropriate dataset for 
Decision tree Regressor 
Aim: To write a program to demonstrate the working of Decision tree Regressor. Use appropriate dataset 
for Decision tree regressor. 
Dataset sample: (height dataset) 
Age,Height 
10,18 
11,138 
12,138 
13,139 
14,139 
... 
... 
Program and Output: 
In: import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
In: data = pd.read_csv('Height_Age_Dataset.csv') 
data 
Out: 
In: data.head() 
Out: 
In: X = data.iloc[:, 0:1].values 
y = data.iloc[:, 1].values 
In: from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 
In: from sklearn.tree import DecisionTreeRegressor 
In: DtReg = DecisionTreeRegressor(random_state = 0) 
In: DtReg.fit(X_train, y_train) 
Out: DecisionTreeRegressor(random_state=0) 
In: y_predict_dtr = DtReg.predict((X_test)) 
In: from sklearn import metrics 
r_square = metrics.r2_score(y_test, y_predict_dtr) 
print('R-Square Error associated with Decision Tree Regression is:', r_square) 
Out: R-Square Error associated with Decision Tree Regression is: -1439.1 
In: X_val = np.arange(min(X_train), max(X_train), 0.01) 
 
In: #Reshape the data into a len(X_val)*1 array in order to make a column out of the X_val values 
X_val = X_val.reshape((len(X_val), 1)) 
 
In: #Define a scatter plot for training data 
plt.scatter(X_train, y_train, color = 'blue') 
 
In: #Plot the predicted data 
plt.plot(X_val, DtReg.predict(X_val), color = 'red') 
 
In: #Define the title 
plt.title('Height prediction using Decision Tree Regression') 
 
In: #Define X axis label 
plt.xlabel('Age') 
 
In: #Define Y axis label 
plt.ylabel('Height') 
In: #Set the size of the plot for better clarity 
plt.figure(figsize=(1,1)) 
 
In: #Draw the plot 
plt.show() 
Out: 
In: #Import export_graphviz package 
from sklearn.tree import export_graphviz 
 
In: #Store the decision tree in a tree.dot file in order to visualize the plot. 
export_graphviz(DtReg, out_file ='dtregression.dot', 
 feature_names =['Age']) 
In: # Predicting Height based on Age using Decision Tree Regression 
height_pred = DtReg.predict([[41]]) 
print("Predicted Height: % d"% height_pred) 
Out: Predicted Height: 143



Experiment name: 
Write a program to demonstrate the working of Random Forest regressor. Use appropriate dataset for 
Random Forest regressor 
Aim: To write a program to demonstrate the working of Random Forest regressor. Use appropriate dataset 
for Random Forest regressor. 
Dataset sample: (iris dataset) 
Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species 
1,5.1,3.5,1.4,0.2,Iris-setosa 
2,4.9,3.0,1.4,0.2,Iris-setosa 
3,4.7,3.2,1.3,0.2,Iris-setosa 
... 
... 
Program and Output: 
In: from sklearn.datasets import load_iris 
iris = load_iris() 
dir(iris) 
Out: ['DESCR', 'data', 'data_module', 'feature_names', 'filename', 'frame', 'target', 'target_names'] 
In: import pandas as pd 
df = pd.DataFrame(iris.data, columns=iris.feature_names) 
df.head() 
Out: 
In: df['target'] = iris.target 
df.head() 
Out: 
In: from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2) 
In: from sklearn.ensemble import RandomForestClassifier 
model = RandomForestClassifier() 
model.fit(X_train, y_train) 
Out: RandomForestClassifier() 
In: model.score(X_test,y_test) 
Out: 0.9666666666666667 
In: model = RandomForestClassifier(n_estimators=40) 
model.fit(X_train, y_train) 
model.score(X_test,y_test) 
Out: 1.0 
In: y_predicted = model.predict(X_test) 
In: from sklearn.metrics import confusion_matrix 
cm = confusion_matrix(y_test, y_predicted) 
cm 
Out: array([[12, 0, 0], 
 [ 0, 7, 0], 
 [ 0, 0, 11]], dtype=int64)
 
 
 
 
Experiment name: 
Write a program to demonstrate the working of Logistic Regression Classifier. Use appropriate dataset for 
Logistic Regression 
Aim: To write a program to demonstrate the working of Logistic Regression Classifier. Use appropriate 
dataset for Logistic Regression. 
Dataset sample: (employee retention dataset)
satisfaction_level,last_evaluation,number_project,average_montly_hours,time_spend_company,Work_ac
cident,left,promotion_last_5years,Department,salary 
0.38,0.53,2,157,3,0,1,0,sales,low 
0.8,0.86,5,262,6,0,1,0,sales,medium 
0.11,0.88,7,272,4,0,1,0,sales,medium 
... 
... 
Program and Output: 
In: import pandas as pd 
from matplotlib import pyplot as plt 
%matplotlib inline 
In: df = pd.read_csv('HR_comma_sep.csv') 
df.head() 
Out: 
satisfact
ion_leve
l 
last_ev
aluatio
n 
number
_projec
t 
average_m
ontly_hour
s 
time_spen
d_compan
y 
Work_
acciden
t 
l
e
ft 
promotion
_last_5year
s 
Depar
tment 
sala
ry 
0 0.38 0.53 2 157 3 0 1 0 sales low 
1
0.80 0.86 5 262 6 0 1 0 sales 
me
diu
m 
2 0.11 0.88 7 272 4 0 1 0 sales me
diu
satisfact
ion_leve
l 
last_ev
aluatio
n 
number
_projec
t 
average_m
ontly_hour
s 
time_spen
d_compan
y 
Work_
acciden
t 
l
e
ft 
promotion
_last_5year
s 
Depar
tment 
sala
ry 
m 
3 0.72 0.87 5 223 5 0 1 0 sales low 
4 0.37 0.52 2 159 3 0 1 0 sales low 
In: left = df[df.left==1] 
left.shape 
Out: (3571, 10) 
In: retained = df[df.left==0] 
retained.shape 
Out: (11428, 10) 
In: df.groupby('left').mean() 
Out: 
In: pd.crosstab(df.salary,df.left).plot(kind='bar') 
Out: 
In: pd.crosstab(df.Department,df.left).plot(kind='bar') 
Out: 
In: subdf = df[['satisfaction_level','average_montly_hours','promotion_last_5years','salary']] 
subdf.head() 
Out: 
In: salary_dummies = pd.get_dummies(subdf.salary, prefix="salary") 
In: df_with_dummies = pd.concat([subdf,salary_dummies],axis='columns') 
In: df_with_dummies.head() 
Out: 
In: df_with_dummies.drop('salary',axis='columns',inplace=True) 
df_with_dummies.head() 
Out: 
In: X = df_with_dummies 
X.head() 
Out: 
In: y = df.left 
In: from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.3) 
In: from sklearn.linear_model import LogisticRegression 
model = LogisticRegression() 
In: model.fit(X_train, y_train) 
Out: LogisticRegression() 
In: model.predict(X_test) 
Out: array([0, 0, 0, ..., 0, 0, 0], dtype=int64) 
In: model.score(X_test,y_test) 
Out: 0.7761904761904762
